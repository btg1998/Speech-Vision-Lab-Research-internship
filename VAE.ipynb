{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Q5lOEhte1gJ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "d5b50573-67de-4350-fbd4-e04dcf2cf4ff"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.3.0.post4 from http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[?25l  Downloading http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl (592.3MB)\n",
            "\u001b[K    98% |███████████████████████████████▋| 584.4MB 46.9MB/s eta 0:00:01"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[K    100% |████████████████████████████████| 592.3MB 31.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4) (1.14.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4) (3.13)\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-0.3.0.post4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0BBQHd072Tx8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "05affb58-f19c-4513-9b53-5356e0b30b02"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torchvision"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 2.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.5)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/24/f53ff6b61b3d728b90934bddb4f03f8ab584a7f49299bf3bde56e2952612/Pillow-5.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.3.0.post4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch->torchvision) (3.13)\n",
            "Installing collected packages: pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.2.0 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f9y3_SsG2DkL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Required Header Files\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vf8IsirmKD3I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Class for Normal Distribution\n",
        "class Normal(object):\n",
        "    def __init__(self, mu, sigma, log_sigma, v=None, r=None):\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma  # either stdev diagonal itself, or stdev diagonal from decomposition\n",
        "        self.logsigma = log_sigma\n",
        "        dim = mu.get_shape()\n",
        "        if v is None:\n",
        "            v = torch.FloatTensor(*dim)\n",
        "        if r is None:\n",
        "            r = torch.FloatTensor(*dim)\n",
        "        self.v = v\n",
        "        self.r = r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y_0GNwj5KMdO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Encoder class consisting of 2 linear layers\n",
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.linear1 = torch.nn.Linear(D_in, H)\n",
        "        self.linear2 = torch.nn.Linear(H, D_out)\n",
        "    # Forward pass function\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        return F.relu(self.linear2(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-6DEdv30KQ5X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " # Decoder Classc consisting of 2 linear layers symmetrical to the Encoder\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.linear1 = torch.nn.Linear(D_in, H)\n",
        "        self.linear2 = torch.nn.Linear(H, D_out)\n",
        "    # Forward pass function\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        return F.relu(self.linear2(x))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VJ-lp3ABKTLo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Variational Auto Encoder class\n",
        "class VAE(torch.nn.Module):\n",
        "    latent_dim = 8 # Hyper - Parameter which can be later tuned\n",
        "    \n",
        "    # Constructor function for the Variational Auto Encoder\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = encoder # An Encoder\n",
        "        self.decoder = decoder # A Decoder\n",
        "        self._enc_mu = torch.nn.Linear(100, 8)  # Mean\n",
        "        self._enc_log_sigma = torch.nn.Linear(100, 8) # Log Std Deviation\n",
        "        \n",
        "       \n",
        "    # Sampling Function\n",
        "    def _sample_latent(self, h_enc):\n",
        "        \"\"\"\n",
        "        Return the latent normal sample z ~ N(mu, sigma^2)\n",
        "        \"\"\"\n",
        "        mu = self._enc_mu(h_enc)\n",
        "        log_sigma = self._enc_log_sigma(h_enc)\n",
        "        sigma = torch.exp(log_sigma)\n",
        "        std_z = torch.from_numpy(np.random.normal(0, 1, size=sigma.size())).float()\n",
        "\n",
        "        self.z_mean = mu\n",
        "        self.z_sigma = sigma\n",
        "\n",
        "        return mu + sigma * Variable(std_z, requires_grad=False)  # Reparameterization trick\n",
        "    # Forward Pass Function\n",
        "    def forward(self, state):\n",
        "        h_enc = self.encoder(state)\n",
        "        z = self._sample_latent(h_enc)\n",
        "        return self.decoder(z)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IUVk1pgZKdna",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to calculate Latent Loss\n",
        "def latent_loss(z_mean, z_stddev):\n",
        "    mean_sq = z_mean * z_mean\n",
        "    stddev_sq = z_stddev * z_stddev\n",
        "    return 0.5 * torch.mean(mean_sq + stddev_sq - torch.log(stddev_sq) - 1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z1ujUI9uKgUS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2166
        },
        "outputId": "1964c2b6-4e1d-48ef-8816-4992b63d8bfe"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    \n",
        "    input_dim = 28 * 28\n",
        "    batch_size = 32\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor()])\n",
        "    mnist = torchvision.datasets.MNIST('./', download=True, transform=transform)\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(mnist, batch_size=batch_size,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "\n",
        "    print('Number of samples: ', len(mnist))\n",
        "    # Intialization of the Encoder and the Decoder\n",
        "    encoder = Encoder(input_dim, 100, 100)\n",
        "    decoder = Decoder(8, 100, input_dim)\n",
        "    vae = VAE(encoder, decoder)\n",
        "    # Using Mean Squared Loss for training\n",
        "    criterion = nn.MSELoss()\n",
        "    # Using Adam Optimizer\n",
        "    optimizer = optim.Adam(vae.parameters(), lr=0.0001)\n",
        "    l = None\n",
        "    # Training for 100 iterations\n",
        "    for epoch in range(100):\n",
        "        for i, data in enumerate(dataloader, 0):\n",
        "            inputs, classes = data\n",
        "            inputs, classes = Variable(inputs.resize_(batch_size, input_dim)), Variable(classes)\n",
        "            optimizer.zero_grad() # Zeroing the Optimizer Gradients\n",
        "            dec = vae(inputs)\n",
        "            ll = latent_loss(vae.z_mean, vae.z_sigma)\n",
        "            loss = criterion(dec, inputs) + ll\n",
        "            # Back Propogation\n",
        "            loss.backward() \n",
        "            optimizer.step()\n",
        "            l = loss.data[0]\n",
        "        print(epoch, l) # Printing the loss\n",
        "    # Showing the created image\n",
        "    plt.imshow(vae(inputs).data[0].numpy().reshape(28, 28), cmap='gray')\n",
        "    plt.show(block=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n",
            "Number of samples:  60000\n",
            "0 0.06542827934026718\n",
            "1 0.06842564791440964\n",
            "2 0.07225073873996735\n",
            "3 0.0666666179895401\n",
            "4 0.0651979073882103\n",
            "5 0.07221096754074097\n",
            "6 0.059068262577056885\n",
            "7 0.06676647067070007\n",
            "8 0.0694689229130745\n",
            "9 0.06773657351732254\n",
            "10 0.066789411008358\n",
            "11 0.07382402569055557\n",
            "12 0.0663641095161438\n",
            "13 0.06821306049823761\n",
            "14 0.06740333884954453\n",
            "15 0.07181020826101303\n",
            "16 0.06951093673706055\n",
            "17 0.07425732165575027\n",
            "18 0.06882365792989731\n",
            "19 0.0683014988899231\n",
            "20 0.07120612263679504\n",
            "21 0.06374897062778473\n",
            "22 0.0691245049238205\n",
            "23 0.0674884170293808\n",
            "24 0.07167474180459976\n",
            "25 0.06827016919851303\n",
            "26 0.06994721293449402\n",
            "27 0.06811806559562683\n",
            "28 0.0673644170165062\n",
            "29 0.0659450888633728\n",
            "30 0.06767313927412033\n",
            "31 0.06886132061481476\n",
            "32 0.06857798248529434\n",
            "33 0.06544660031795502\n",
            "34 0.0727199837565422\n",
            "35 0.06401597708463669\n",
            "36 0.06678040325641632\n",
            "37 0.06609135121107101\n",
            "38 0.06844674050807953\n",
            "39 0.06947821378707886\n",
            "40 0.061921533197164536\n",
            "41 0.06765604019165039\n",
            "42 0.07043182849884033\n",
            "43 0.06361844390630722\n",
            "44 0.06962186098098755\n",
            "45 0.0636453926563263\n",
            "46 0.06777553260326385\n",
            "47 0.06922657042741776\n",
            "48 0.06119045615196228\n",
            "49 0.06953226774930954\n",
            "50 0.06745919585227966\n",
            "51 0.06140414997935295\n",
            "52 0.06889957934617996\n",
            "53 0.0711568146944046\n",
            "54 0.06769779324531555\n",
            "55 0.06686205416917801\n",
            "56 0.061267562210559845\n",
            "57 0.061634309589862823\n",
            "58 0.0605134479701519\n",
            "59 0.06736715137958527\n",
            "60 0.06669361144304276\n",
            "61 0.07155981659889221\n",
            "62 0.06561967730522156\n",
            "63 0.06942518055438995\n",
            "64 0.06829691678285599\n",
            "65 0.07353446632623672\n",
            "66 0.06792635470628738\n",
            "67 0.06735415756702423\n",
            "68 0.06425178796052933\n",
            "69 0.06748094409704208\n",
            "70 0.06715362519025803\n",
            "71 0.06715904921293259\n",
            "72 0.06606080383062363\n",
            "73 0.06483520567417145\n",
            "74 0.06952353566884995\n",
            "75 0.060481104999780655\n",
            "76 0.06725474447011948\n",
            "77 0.06707185506820679\n",
            "78 0.07053596526384354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "79 0.06346061080694199\n",
            "80 0.07050180435180664\n",
            "81 0.06437322497367859\n",
            "82 0.06957194209098816\n",
            "83 0.0688934326171875\n",
            "84 0.07204177230596542\n",
            "85 0.06611320376396179\n",
            "86 0.06474726647138596\n",
            "87 0.06333782523870468\n",
            "88 0.06352560967206955\n",
            "89 0.061358191072940826\n",
            "90 0.06986640393733978\n",
            "91 0.06939544528722763\n",
            "92 0.059986915439367294\n",
            "93 0.06659296154975891\n",
            "94 0.06321022659540176\n",
            "95 0.06904921680688858\n",
            "96 0.06783326715230942\n",
            "97 0.06502271443605423\n",
            "98 0.06741303950548172\n",
            "99 0.06863559782505035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fb647661390>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "bgsRZX-nYfB6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The produced image seems to be a mix of 3 and 9, better results can be obtained from sophisticated architectures and training for longer epochs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fJNSWkjZZoBA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4663a7ad-f5de-4026-f43e-6c8b6a38cc90"
      },
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "Qt0BceCsZovq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}